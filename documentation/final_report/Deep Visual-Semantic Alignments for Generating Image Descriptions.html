<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Deep Visual-Semantic Alignments for Generating Image Descriptions</title>

  <!-- bootstrap -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap-theme.min.css">

  <!-- Google fonts -->
  <link href='http://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>

  <!-- Google Analytics -->

  <link rel="stylesheet" type="text/css" href="style.css" />

  <script>
  function page_loaded() {
  }
  </script>
</head>

<body onload="page_loaded()">

<div id="header">
  <a href="http://vision.stanford.edu/">
    <img src="visionlablogo.png" style="height:50px; float: left; margin-left: 20px;">
  </a>
  <a href="http://stanford.edu/">
    <img src="stanfordlogo.jpg" style="height:50px; float: right; margin-right: 20px;">
  </a>
  <h1>Deep Visual-Semantic Alignments for Generating Image Descriptions</h1>
  <div style="clear:both;"></div>
</div>

<!--
<div id="teaser">
</div>
-->

<div class="sechighlight">
<div class="container sec">
  <h2>Abstract</h2>

  <div id="coursedesc">
      We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.
  </div>
</div>
</div>

<div class="container sec">
<h2>CVPR 2015 Paper</h2>
<a href="../cvpr2015.pdf" style="font-size:18px">Deep Visual-Semantic Alignments for Generating Image Descriptions</a><br>

<a href="http://cs.stanford.edu/people/karpathy/">Andrej Karpathy</a>, <a href="http://vision.stanford.edu/">Li Fei-Fei</a><br>

</div>

<div class="sechighlight">
<div class="container sec">
  <div class="row">
    <div class="col-md-4">
      <h2>Code</h2>
      See our code release on <a href="https://github.com/karpathy/neuraltalk">Github</a>, which allows you to train Multimodal Recurrent Neural Networks that describe images with sentences. You may also want to download the dataset JSON and VGG CNN features for <a href="flickr8k.zip">Flickr8K</a> (50MB), <a href="flickr30k.zip">Flickr30K</a> (200MB), or <a href="coco.zip">COCO</a> (750MB). You can also download the JSON blobs for all three datasets (but without the VGG CNN features) <a href="caption_datasets.zip">here</a> (35MB). See our Github repo for more instructions.
      <br>
      Update: NeuralTalk has now been <b>deprecated</b>, in favor of the more recent release of <a href="https://github.com/karpathy/neuraltalk2">NeuralTalk2</a>.
    </div>
    <div class="col-md-4">
      <h2>Retrieval Demo</h2>
      Our full retrival results for a test set of 1,000 COCO images can be found in this <a href="http://cs.stanford.edu/people/karpathy/deepimagesent/rankingdemo/">interactive retrieval web demo</a>. 
    </div>
    <div class="col-md-4">
      <h2>Region Annotations</h2>
      Our COCO region annotations test set can be found <a href="amt_region_results.json">here as json</a>. These consist of 9000 noun phrases collected on 200 images from COCO. Every image has a total of 45 region annotations from 9 distinct AMT workers.
    </div>
  </div>
</div>
</div>

<div class="container sec">
  <h2>Multimodal Recurrent Neural Network</h2>
  Our Multimodal Recurrent Neural Architecture generates sentence descriptions from images. Below are a few examples of generated sentences:<br><br>
  <div class="row rnndemo">
    <div class="col-sm-3">
      <img src="guitar.png" class="demoimg">
      <br>
      "man in black shirt is playing guitar."
    </div>
    <div class="col-sm-3">
      <img src="worker.png" class="demoimg">
      <br>
      "construction worker in orange safety vest is working on road."
    </div>
    <div class="col-sm-3">
      <img src="legos.png" class="demoimg">
      <br>
      "two young girls are playing with lego toy."
    </div>
    <div class="col-sm-3">
      <img src="backflip.png" class="demoimg">
      <br>
      "boy is doing backflip on wakeboard."
    </div>
  </div>

  <div class="row rnndemo">
    <div class="col-sm-3">
      <img src="girl.png" class="demoimg">
      <br>
      "girl in pink dress is jumping in air."
    </div>
    <div class="col-sm-3">
      <img src="dogbar.png" class="demoimg">
      <br>
      "black and white dog jumps over bar."
    </div>
    <div class="col-sm-3">
      <img src="girlswing.png" class="demoimg">
      <br>
      "young girl in pink shirt is swinging on swing."
    </div>
    <div class="col-sm-3">
      <img src="surf.png" class="demoimg">
      <br>
      "man in blue wetsuit is surfing on wave."
    </div>
  </div>

  <div class="row rnndemo">
    <div class="col-sm-3">
      <img src="girlcake.png" class="demoimg">
      <br>
      "little girl is eating piece of cake."
    </div>
    <div class="col-sm-3">
      <img src="baseball.png" class="demoimg">
      <br>
      "baseball player is throwing ball in game."
    </div>
    <div class="col-sm-3">
      <img src="bananas.png" class="demoimg">
      <br>
      "woman is holding bunch of bananas."
    </div>
    <div class="col-sm-3">
      <img src="cat.png" class="demoimg">
      <br>
      "black cat is sitting on top of suitcase."
    </div>
  </div>

  <div class="row rnndemo">
    <div class="col-sm-3">
      <img src="bat.jpeg" class="demoimg">
      <br>
      "a young boy is holding a baseball bat."
    </div>
    <div class="col-sm-3">
      <img src="catremote.jpeg" class="demoimg">
      <br>
      "a cat is sitting on a couch with a remote control."
    </div>
    <div class="col-sm-3">
      <img src="teddy.jpeg" class="demoimg">
      <br>
      "a woman holding a teddy bear in front of a mirror."
    </div>
    <div class="col-sm-3">
      <img src="horse.jpeg" class="demoimg">
      <br>
      "a horse is standing in the middle of a road."
    </div>
  </div>

  <br><br> See web demo with many more captioning results <a href="http://cs.stanford.edu/people/karpathy/deepimagesent/generationdemo/">here</a>
</div>

<div class="sechighlight">
<div class="container sec">
  <h2>Visual-Semantic Alignments</h2>
  Our alignment model learns to associate images and snippets of text. Below are a few examples of inferred alignments. For each image, the model retrieves the most compatible sentence and grounds its pieces in the image. We show the grounding as a line to the center of the corresponding bounding box. Each box has a single but arbitrary color.<br><br>

  <div class="row rnndemo">
    <div class="col-sm-4">
      <div class="demodiv2">
        <img src="dogball.png" class="demoimg2">
      </div>
    </div>
    <div class="col-sm-4">
      <div class="demodiv2">
        <img src="accordion.png" class="demoimg2">
      </div>
    </div>
    <div class="col-sm-4">
      <div class="demodiv2">
        <img src="tennis.png" class="demoimg2">
      </div>
    </div>
  </div>

  <br>
  See many more examples <a href="http://cs.stanford.edu/people/karpathy/deepimagesent/rankingdemo/">here</a>.

</div>
</div>

<div class="sechighlight">
<div id="footer">
We gratefully acknowledge the support of <b>NVIDIA</b> Corporation with the donation of the GPUs used for this research.<br>
We are also thankful to <b>Yahoo</b> for their generous donation of cluster machines used in this research.<br>
This research is partially supported by an ONR MURI grant, and NSF ISS-1115313.
</div>
</div>

<!-- jQuery and Boostrap -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>

<!-- Analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-3698471-13', 'auto');
  ga('send', 'pageview');

</script>

</body>

</html>
