default_configuration:
  model_type: 'yolov8m'
  seed: 42 # Seed for reproducibility
  model_directory: 'models' # Directory to save the model
  load_saved_model: True  # Load saved models from the model directory instead of training
  device: "cpu" # Device to run the model on. cpu is standard for most, mps available for Apple Silicon with Metal Performance Shaders (MPS). Select 0 or 1 for GPU 
  data_split: 
    train_split: 0.6 # Train split
    validation_split: 0.2 # Validation split
    test_split: 0.2 # Test split
  training_parameters: # 
    epochs: 10
    batch: 64 # Batch size 
    imgsz: 320 # Image size (width and height) for training
    cache: 'ram' # Cache images for faster training. Options: 'ram' or 'disk'. 'ram' is faster but uses more memory. 'disk' is slower but uses less memory. 'ram' is not deterministic.
    time: 2 # Max time for training, in hours
    plots: True # Plot training results
    exist_ok:           True   # If True, allows overwriting of an existing project/name directory. Useful for iterative experimentation without needing to manually clear previous outputs. Directory management is handled by the code, so keep this true
    #### The below are defaults. Just putting them for completeness and ease of access 
    patience:           100     # Number of epochs to wait without improvement in validation metrics before early stopping the training. Helps prevent overfitting by stopping training when performance plateaus.
    save:               True    # Save the model after training
    save_period:        2      # Frequency of saving model checkpoints, specified in epochs. A value of -1 disables this feature. Useful for saving interim models during long training sessions.
    workers:            8       # Number of worker threads for data loading (per RANK if Multi-GPU training). Influences the speed of data preprocessing and feeding into the model, especially useful in multi-GPU setups.
    pretrained:         True # Determines whether to start training from a pretrained model. Can be a boolean value or a string path to a specific model from which to load weights. Enhances training efficiency and model performance.
    optimizer:         'auto' # Choice of optimizer for training. Options include SGD, Adam, AdamW, NAdam, RAdam, RMSProp etc., or auto for automatic selection based on model configuration. Affects convergence speed and stability.
    deterministic:      True    # Forces deterministic algorithm use, ensuring reproducibility but may affect performance and speed due to the restriction on non-deterministic algorithms.
    single_cls: True   # Treats all classes in multi-class datasets as a single class during training. Useful for binary classification tasks or when focusing on object presence rather than classification.
    rect:               False   # Enables rectangular training, optimizing batch composition for minimal padding. Can improve efficiency and speed but may affect model accuracy.
    cos_lr:             False   # Utilizes a cosine learning rate scheduler, adjusting the learning rate following a cosine curve over epochs. Helps in managing learning rate for better convergence.
    close_mosaic:       10      # Disables mosaic data augmentation in the last N epochs to stabilize training before completion. Setting to 0 disables this feature.
    resume:             True   # Resumes training from the last saved checkpoint. Automatically loads model weights, optimizer state, and epoch count, continuing training seamlessly.
    amp:                True    # Enables Automatic Mixed Precision (AMP) training, reducing memory usage and possibly speeding up training with minimal impact on accuracy.
    fraction:           1.0     # Specifies the fraction of the dataset to use for training. Allows for training on a subset of the full dataset, useful for experiments or when resources are limited.
    profile:            False   # Enables profiling of ONNX and TensorRT speeds during training, useful for optimizing model deployment.
    freeze:             None    # Freezes the first N layers of the model or specified layers by index, reducing the number of trainable parameters. Useful for fine-tuning or transfer learning.
    lr0:                0.01    # Initial learning rate (i.e. SGD=1E-2, Adam=1E-3) . Adjusting this value is crucial for the optimization process, influencing how rapidly model weights are updated.
    lrf:                0.01    # Final learning rate as a fraction of the initial rate = (lr0 * lrf), used in conjunction with schedulers to adjust the learning rate over time.
    momentum:           0.937   # Momentum factor for SGD or beta1 for Adam optimizers, influencing the incorporation of past gradients in the current update.
    weight_decay:       0.0005  # L2 regularization term, penalizing large weights to prevent overfitting.
    warmup_epochs:      3.0     # Number of epochs for learning rate warmup, gradually increasing the learning rate from a low value to the initial learning rate to stabilize training early on.
    warmup_momentum:    0.8     # Initial momentum for warmup phase, gradually adjusting to the set momentum over the warmup period.
    warmup_bias_lr:     0.1     # Learning rate for bias parameters during the warmup phase, helping stabilize model training in the initial epochs.
    box:                7.5     # Weight of the box loss component in the loss function, influencing how much emphasis is placed on accurately predicting bounding box coordinates.
    cls:                0.5     # Weight of the classification loss in the total loss function, affecting the importance of correct class prediction relative to other components.
    dfl:                1.5     # Weight of the distribution focal loss, used in certain YOLO versions for fine-grained classification.
    pose:               12.0    # Weight of the pose loss in models trained for pose estimation, influencing the emphasis on accurately predicting pose keypoints.
    kobj:               2.0     # Weight of the keypoint objectness loss in pose estimation models, balancing detection confidence with pose accuracy.
    nbs:                64      # Nominal batch size for normalization of loss.
    overlap_mask:       True    # Determines whether object masks should be merged into a single mask for training, or kept separate for each object. In case of overlap, the smaller mask is overlayed on top of the larger mask during merge.
    mask_ratio:         4       # Downsample ratio for segmentation masks, affecting the resolution of masks used during training.
    dropout:            0.0     # Dropout rate for regularization in classification tasks, preventing overfitting by randomly omitting units during training.
    val:                True    # Enables validation during training, allowing for periodic evaluation of model performance on a separate dataset.
  validation_parameters: 
    bypass_validation: False # Bypass validation and only run training
    imgsz: 320 # Image size (width and height). All images resized to this 
    conf: 0.001 # Object confidence threshold 
    max_det: 100 # Maximum number of detections per image
    iou: 0.6 # IOU threshold for NMS
    save_json: True # Save detections as JSON
    save_hybrid: True # Save detections as hybrid
    plots: True # Plot detections. Generates and savess plots of predictions vs. ground truth 
    exist_ok:           True   # If True, allows overwriting of an existing project/name directory. Useful for iterative experimentation without needing to manually clear previous outputs. Directory management is handled by the code, so keep this true
  doc: Note that anything that is not specified in the individual model will be taken from here. Almost all parameters from here can be overridden in the individual model configuration. See https://docs.ultralytics.com/modes/train/#train-settings for more details.
model_metadata: 
  model_a: 
    model_type: 'yolov8m'
    training_parameters: 
      epochs: 10